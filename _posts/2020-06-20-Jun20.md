---
  categories:
    -Blog
  title:  "June 20"
  hidden: true
  author_profile: false
---

Today I ordered â€œRemote: Office Not Requiredâ€ and â€œReady Player Oneâ€ since Iâ€™m trying to read more physical books. While waiting for them to arrive Iâ€™ll read â€œFreakonomicsâ€ and â€œThe Mythical Man Monthâ€.  

I had a dream about cookies. The clouds were big, soft, fluffy, crunchy, chewy, tasty cookies. I was flying towards them on a cookie disc that I was steering using reigns (like a horse). I woke up and got cookies. 

We had ras malai today. It was steaming hot, the milk was sweet and creamy, the dumplings were soft. The bowl was empty before I knew it. I enjoyed it a lot. 

Appleâ€™s World Wide Developer Conference starts on Monday. Iâ€™m not a big Apple fan, but after getting to try a MacBook Pro for three months thanks to my internship with SWVL I could see the appeal. Still donâ€™t see the point of any iPhones newer than my 6s though. Iâ€™m a light phone user. 

I read a [really good article](https://stratechery.com/2020/apple-arm-and-intel/) that talks about the change Iâ€™m most excited about: a potential transition from x86 to ARM chips. I mean it when I say really good, itâ€™s easily above the 95th percentile.  

Iâ€™ve gone to sleep thinking about how cool this is, and then continued to think about how cool this is in the morning shower. You might think itâ€™s because of this: 

> ARM Macs will have superior performance to Intel Macs on both a per-watt basis and a per-dollar basis. That means that the next version of the MacBook Air, for example, could be cheaper even as it has better battery life and far better performance  

But thatâ€™s not what Iâ€™m obsessing over. Itâ€™s this: 

> ARM on Mac, particularly for developers, could be a radical change indeed that ends up transforming the server space. 

I remember being a little disappointed back when Cloudflare told us about their new [Gen X: Intel Not Inside](https://blog.cloudflare.com/cloudflares-gen-x-servers-for-an-accelerated-future/) datacenters. It turns out that theyâ€™d switched to AMD (which is an improvement, although still x86). An Excerpt: 

> Readers of our blog might remember our excitement around ARM processors. We even ported the entirety of our software stack to run on ARM, just as it does with x86, and have been maintaining that ever since even though it calls for slightly more work for our software engineering teams. We did this leading up to the launch of Qualcommâ€™s Centriq server CPU, which eventually got shuttered. While none of the off-the-shelf ARM CPUs available this moment are interesting to us, we remain optimistic about high core count offerings launching in 2020 and beyond, and look forward to a day when our servers are a mix of x86 (Intel and AMD) and ARM. 

I think that knowing how to translate high performance software from x86 to ARM is a skill that will explode in value and demand in the near future. Emulation hurts performance, and there's no 1-1 mapping from a binary in one instruction set to another, so I'm curious about what clever tricks compiler writers will come up with to deal with this problem. LLVM back end guys must be going crazy. I've been on the [architecture's wikipedia page](https://en.wikipedia.org/wiki/ARM_architecture), and ARM assembly is so *different* from what I learned when going through CMU's Computer Systems course. Conditional execution blew my mind (so elegant! so concise!).

The increase in power efficiency makes this a good move for the environment, as computing infrastructure running ARM will have a reduced carbon footprint.  

ARM is already popular on mobile, and so recently Iâ€™ve been diving a little deeper into machine learning on edge devices. Imagine writing a thin wrapper around embedded ML code to deploy it to a large IoT cluster. Itâ€™s completely reversing the natural order of things and I love it. I want to find out more about the M-Profile Vector Extension (MVE, AKA Helium), which is a new vector instruction set extension for machine learning and signal processing. Of course, I need to learn the basics first, haha.

Letâ€™s be clear: at the end of the day most deep learning still boils down to matrix multiplication (with non-linear activation functions applied in between), so this isnâ€™t going to dethrone Nvidia GPUs. Iâ€™m missing some details about the parallel algorithms involved which explain exactly why it could be unrealistic to scale training to a swarm/network (probably due to problems with memory bandwidth?). In general CPUs arenâ€™t designed to do the sort of repetitive tasks that GPUs do, but nobody knows for sure how thatâ€™ll apply to dividing tasks among *many* processors in a distributed system.  

And we can still [speed up neural networks on CPUs](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/37631.pdf) (that paper was written for x86 chips, so whoever writes the ARM version will be rolling in the citations). Mobile GPUs are way weaker than server/desktop GPUs, but the same disparity doesnâ€™t apply when it comes to CPUs now (well, except for power/thermal concerns... I feel like erasing the last four paragraphs since Iâ€™m clearly not an expert on this). I know so little! 

Iâ€™m still learning stuff, so if what I just wrote was confusing, itâ€™s my fault and Iâ€™d appreciate it if you could let me know where Iâ€™m mistaken so that I can correct my misunderstanding. In a nutshell, CPUs still matter on the AI stack, and CPUs are getting better, and that makes AI better, which makes me happy. 

I like language and time-series based stuff more than images, where CPUs still shine. Side note: Itâ€™s funny to read [Nvidiaâ€™s](https://blogs.nvidia.com/blog/2009/12/16/whats-the-difference-between-a-cpu-and-a-gpu/) vs [Intelâ€™s](https://www.intel.com/content/www/us/en/products/docs/processors/cpu-vs-gpu.html) explanations of GPUs vs CPUs. Their biases are apparent ðŸ˜Š 

Anyway, my family is being too loud for me to focus right now, so Iâ€™ll have to wrap up. So many ideas to write about! Not enough people to talk to them about! I need to focus but everything is too interesting. Itâ€™s really weird that whatever minor life events I write about in this are immortalized, but everything else that isnâ€™t contained within these pages decays within my meagre memory.  

 

 

 
